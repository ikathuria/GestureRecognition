<img align="center" src="https://researchoutreach.org/wp-content/uploads/2020/05/Celso-de-Melo-Kazunori-Terada-Main-Image.jpg" />

<h1 align="center">ğŸ„¶ğŸ„´ğŸ…‚ğŸ…ƒğŸ…„ğŸ…ğŸ„´ ğŸ…ğŸ„´ğŸ„²ğŸ„¾ğŸ„¶ğŸ„½ğŸ„¸ğŸ…ƒğŸ„¸ğŸ„¾ğŸ„½</h1>

## Using the model
1. Clone the repository using git  
`git clone https://github.com/ikathuria/GestureRecognition.git`

2. Navigate to that folder  
`cd GestureRecognition`

3. Install all the dependencies  
`pip install -r requirements.txt`

4. Run the visualization file to see the outputs  
`python visualization.py`

## The gestures
<img src="data/zero/969.jpg">
<img src="data/one/969.jpg">
<img src="data/two/969.jpg">
<img src="data/three/969.jpg">
<img src="data/four/969.jpg">
<img src="data/five/969.jpg">
<img src="data/six/969.jpg">
<img src="data/seven/969.jpg">
<img src="data/eight/969.jpg">
<img src="data/nine/969.jpg">
<img src="data/up/969.jpg">
<img src="data/down/969.jpg">
<img src="data/left/969.jpg">
<img src="data/right/969.jpg">
<img src="data/off/969.jpg">
<img src="data/on/969.jpg">
<img src="data/ok/969.jpg">
<img src="data/blank/969.jpg">


## TODO
Add depth estimation
